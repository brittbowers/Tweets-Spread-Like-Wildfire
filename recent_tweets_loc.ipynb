{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import preprocessor as p \n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "df_pge = pge blackout trending\n",
    "\n",
    "df_wf = wildfire trending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pge = pd.read_csv('data/pge_shutdown.csv', delimiter = ',', quotechar = '|', \n",
    "                 names = ['date', 'tweet', 'users', 'followers', 'location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>users</th>\n",
       "      <th>followers</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-19 02:34:26</td>\n",
       "      <td>b'Im gonna miss those very distant lights soon...</td>\n",
       "      <td>girloneonegirl</td>\n",
       "      <td>725</td>\n",
       "      <td>Yer moms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-11-19 02:34:13</td>\n",
       "      <td>b'I have empathy for people affected by the we...</td>\n",
       "      <td>mr_goblins948</td>\n",
       "      <td>1812</td>\n",
       "      <td>California, USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-11-19 02:26:24</td>\n",
       "      <td>b'at least my house wont be affected, right? #...</td>\n",
       "      <td>Xxepicgamer_xX</td>\n",
       "      <td>216</td>\n",
       "      <td>North Siberian Gulag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019-11-19 02:14:47</td>\n",
       "      <td>b'Here we go again. Every damn time I go to co...</td>\n",
       "      <td>Quesozitto</td>\n",
       "      <td>33</td>\n",
       "      <td>I'm lost too.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019-11-19 02:10:15</td>\n",
       "      <td>b'@lakatzzz @RedwoodGirl @yellowsnpdragon Ridi...</td>\n",
       "      <td>Cheryl4SaveCali</td>\n",
       "      <td>2117</td>\n",
       "      <td>Sacramento, CA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                              tweet  \\\n",
       "0  2019-11-19 02:34:26  b'Im gonna miss those very distant lights soon...   \n",
       "1  2019-11-19 02:34:13  b'I have empathy for people affected by the we...   \n",
       "2  2019-11-19 02:26:24  b'at least my house wont be affected, right? #...   \n",
       "3  2019-11-19 02:14:47  b'Here we go again. Every damn time I go to co...   \n",
       "4  2019-11-19 02:10:15  b'@lakatzzz @RedwoodGirl @yellowsnpdragon Ridi...   \n",
       "\n",
       "             users  followers               location  \n",
       "0   girloneonegirl        725               Yer moms  \n",
       "1    mr_goblins948       1812        California, USA  \n",
       "2   Xxepicgamer_xX        216  North Siberian Gulag   \n",
       "3       Quesozitto         33          I'm lost too.  \n",
       "4  Cheryl4SaveCali       2117         Sacramento, CA  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wf = pd.read_csv('data/wildfire_recent.csv', delimiter = ',', quotechar = '|', \n",
    "                 names = ['date', 'tweet', 'users', 'followers', 'location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>users</th>\n",
       "      <th>followers</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-19 18:15:35</td>\n",
       "      <td>Sad. Because of this Summer's wildfire and sub...</td>\n",
       "      <td>RyanBernhart_Wx</td>\n",
       "      <td>36</td>\n",
       "      <td>Maricopa, AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-11-19 18:15:16</td>\n",
       "      <td>Huge Flow Country wildfire 'doubled #Scotland'...</td>\n",
       "      <td>CWL_BeGreen</td>\n",
       "      <td>1043</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-11-19 18:14:42</td>\n",
       "      <td>Suburban sprawl and climate change complicate ...</td>\n",
       "      <td>babday</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019-11-19 18:13:39</td>\n",
       "      <td>In 2018, over 8 million acres were burned by w...</td>\n",
       "      <td>FavaFinancialGr</td>\n",
       "      <td>188</td>\n",
       "      <td>Totowa,  New Jersey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019-11-19 18:13:37</td>\n",
       "      <td>In 2018, over 8 million acres were burned by w...</td>\n",
       "      <td>ElkAgencyIns</td>\n",
       "      <td>12</td>\n",
       "      <td>Elk River, Minnesota</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                              tweet  \\\n",
       "0  2019-11-19 18:15:35  Sad. Because of this Summer's wildfire and sub...   \n",
       "1  2019-11-19 18:15:16  Huge Flow Country wildfire 'doubled #Scotland'...   \n",
       "2  2019-11-19 18:14:42  Suburban sprawl and climate change complicate ...   \n",
       "3  2019-11-19 18:13:39  In 2018, over 8 million acres were burned by w...   \n",
       "4  2019-11-19 18:13:37  In 2018, over 8 million acres were burned by w...   \n",
       "\n",
       "             users  followers              location  \n",
       "0  RyanBernhart_Wx         36          Maricopa, AZ  \n",
       "1      CWL_BeGreen       1043                   NaN  \n",
       "2           babday          2                   NaN  \n",
       "3  FavaFinancialGr        188   Totowa,  New Jersey  \n",
       "4     ElkAgencyIns         12  Elk River, Minnesota  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9662, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase\n",
    "#df_pge['tweet'] = [x.lower() for x in df_pge['tweet']]\n",
    "df_wf['tweet'] = [x.lower() for x in df_wf['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'like wildfire' \n",
    "for i,tweet in enumerate(df_wf['tweet']):\n",
    "    if 'like wildfire' in tweet:\n",
    "        df_wf = df_wf.drop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8970, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize using preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [p.tokenize(tweet) for tweet in df_wf['tweet']]\n",
    "parsed_tweet = [p.parse(tweet) for tweet in df_wf['tweet']]\n",
    "clean = [p.clean(tweet) for tweet in df_wf['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates?\n",
    "df_wf = df_wf.drop_duplicates(subset = 'tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wf = df_wf.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wf['emojis'] = [tweet.emojis for tweet in parsed_tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wf['hashtags'] = [tweet.hashtags for tweet in parsed_tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United States          120\n",
       "California, USA         89\n",
       "London, England         89\n",
       "New York, NY            88\n",
       "Los Angeles, CA         86\n",
       "                      ... \n",
       "in your dreams...        1\n",
       "Marble Falls, TX         1\n",
       "Bangladesh 🇧🇩            1\n",
       "Hamilton, ON CANADA      1\n",
       "Walt Disney Land         1\n",
       "Name: location, Length: 3028, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wf.location.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "    \n",
    "    # Remove mentions, hashtags, urls\n",
    "    mytokens = [word for word in mytokens if '$' not in word]\n",
    "    \n",
    "    # Remove '...'\n",
    "    mytokens = [word for word in mytokens if '…' not in word]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wf['clean_tweet'] = [' '.join(spacy_tokenizer(tweet)) for tweet in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wf['tokens'] = [tweet.split(' ') for tweet in df_wf['clean_tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>users</th>\n",
       "      <th>followers</th>\n",
       "      <th>location</th>\n",
       "      <th>emojis</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-19 18:15:35</td>\n",
       "      <td>sad. because of this summer's wildfire and sub...</td>\n",
       "      <td>RyanBernhart_Wx</td>\n",
       "      <td>36</td>\n",
       "      <td>Maricopa, AZ</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>sad summer wildfire subsequent flash flooding ...</td>\n",
       "      <td>[sad, summer, wildfire, subsequent, flash, flo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-11-19 18:15:16</td>\n",
       "      <td>huge flow country wildfire 'doubled #scotland'...</td>\n",
       "      <td>CWL_BeGreen</td>\n",
       "      <td>1043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[(36:45) =&gt; #scotland]</td>\n",
       "      <td>huge flow country wildfire doubled emissions m...</td>\n",
       "      <td>[huge, flow, country, wildfire, doubled, emiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-11-19 18:14:42</td>\n",
       "      <td>suburban sprawl and climate change complicate ...</td>\n",
       "      <td>babday</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>suburban sprawl climate change complicate wild...</td>\n",
       "      <td>[suburban, sprawl, climate, change, complicate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019-11-19 18:13:39</td>\n",
       "      <td>in 2018, over 8 million acres were burned by w...</td>\n",
       "      <td>FavaFinancialGr</td>\n",
       "      <td>188</td>\n",
       "      <td>Totowa,  New Jersey</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018 8 million acres burned wildfire working a...</td>\n",
       "      <td>[2018, 8, million, acres, burned, wildfire, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019-11-19 18:13:37</td>\n",
       "      <td>in 2018, over 8 million acres were burned by w...</td>\n",
       "      <td>ElkAgencyIns</td>\n",
       "      <td>12</td>\n",
       "      <td>Elk River, Minnesota</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018 8 million acres burned wildfire working a...</td>\n",
       "      <td>[2018, 8, million, acres, burned, wildfire, wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                              tweet  \\\n",
       "0  2019-11-19 18:15:35  sad. because of this summer's wildfire and sub...   \n",
       "1  2019-11-19 18:15:16  huge flow country wildfire 'doubled #scotland'...   \n",
       "2  2019-11-19 18:14:42  suburban sprawl and climate change complicate ...   \n",
       "3  2019-11-19 18:13:39  in 2018, over 8 million acres were burned by w...   \n",
       "4  2019-11-19 18:13:37  in 2018, over 8 million acres were burned by w...   \n",
       "\n",
       "             users  followers              location emojis  \\\n",
       "0  RyanBernhart_Wx         36          Maricopa, AZ   None   \n",
       "1      CWL_BeGreen       1043                   NaN   None   \n",
       "2           babday          2                   NaN   None   \n",
       "3  FavaFinancialGr        188   Totowa,  New Jersey   None   \n",
       "4     ElkAgencyIns         12  Elk River, Minnesota   None   \n",
       "\n",
       "                 hashtags                                        clean_tweet  \\\n",
       "0                    None  sad summer wildfire subsequent flash flooding ...   \n",
       "1  [(36:45) => #scotland]  huge flow country wildfire doubled emissions m...   \n",
       "2                    None  suburban sprawl climate change complicate wild...   \n",
       "3                    None  2018 8 million acres burned wildfire working a...   \n",
       "4                    None  2018 8 million acres burned wildfire working a...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [sad, summer, wildfire, subsequent, flash, flo...  \n",
       "1  [huge, flow, country, wildfire, doubled, emiss...  \n",
       "2  [suburban, sprawl, climate, change, complicate...  \n",
       "3  [2018, 8, million, acres, burned, wildfire, wo...  \n",
       "4  [2018, 8, million, acres, burned, wildfire, wo...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,tweet in enumerate(df_wf['clean_tweet']):\n",
    "    if 'michael' in tweet:\n",
    "        df_wf = df_wf.drop(i)\n",
    "    if 'youngadultfemalevocalistoftheyear' in tweet:\n",
    "        df_wf = df_wf.drop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wf = df_wf.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8792, 9)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>users</th>\n",
       "      <th>followers</th>\n",
       "      <th>location</th>\n",
       "      <th>emojis</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-19 18:15:35</td>\n",
       "      <td>sad. because of this summer's wildfire and sub...</td>\n",
       "      <td>RyanBernhart_Wx</td>\n",
       "      <td>36</td>\n",
       "      <td>Maricopa, AZ</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>sad summer wildfire subsequent flash flooding ...</td>\n",
       "      <td>[sad, summer, wildfire, subsequent, flash, flo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-11-19 18:15:16</td>\n",
       "      <td>huge flow country wildfire 'doubled #scotland'...</td>\n",
       "      <td>CWL_BeGreen</td>\n",
       "      <td>1043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[(36:45) =&gt; #scotland]</td>\n",
       "      <td>huge flow country wildfire doubled emissions m...</td>\n",
       "      <td>[huge, flow, country, wildfire, doubled, emiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-11-19 18:14:42</td>\n",
       "      <td>suburban sprawl and climate change complicate ...</td>\n",
       "      <td>babday</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>suburban sprawl climate change complicate wild...</td>\n",
       "      <td>[suburban, sprawl, climate, change, complicate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019-11-19 18:13:39</td>\n",
       "      <td>in 2018, over 8 million acres were burned by w...</td>\n",
       "      <td>FavaFinancialGr</td>\n",
       "      <td>188</td>\n",
       "      <td>Totowa,  New Jersey</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018 8 million acres burned wildfire working a...</td>\n",
       "      <td>[2018, 8, million, acres, burned, wildfire, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019-11-19 18:13:37</td>\n",
       "      <td>in 2018, over 8 million acres were burned by w...</td>\n",
       "      <td>ElkAgencyIns</td>\n",
       "      <td>12</td>\n",
       "      <td>Elk River, Minnesota</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018 8 million acres burned wildfire working a...</td>\n",
       "      <td>[2018, 8, million, acres, burned, wildfire, wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                              tweet  \\\n",
       "0  2019-11-19 18:15:35  sad. because of this summer's wildfire and sub...   \n",
       "1  2019-11-19 18:15:16  huge flow country wildfire 'doubled #scotland'...   \n",
       "2  2019-11-19 18:14:42  suburban sprawl and climate change complicate ...   \n",
       "3  2019-11-19 18:13:39  in 2018, over 8 million acres were burned by w...   \n",
       "4  2019-11-19 18:13:37  in 2018, over 8 million acres were burned by w...   \n",
       "\n",
       "             users  followers              location emojis  \\\n",
       "0  RyanBernhart_Wx         36          Maricopa, AZ   None   \n",
       "1      CWL_BeGreen       1043                   NaN   None   \n",
       "2           babday          2                   NaN   None   \n",
       "3  FavaFinancialGr        188   Totowa,  New Jersey   None   \n",
       "4     ElkAgencyIns         12  Elk River, Minnesota   None   \n",
       "\n",
       "                 hashtags                                        clean_tweet  \\\n",
       "0                    None  sad summer wildfire subsequent flash flooding ...   \n",
       "1  [(36:45) => #scotland]  huge flow country wildfire doubled emissions m...   \n",
       "2                    None  suburban sprawl climate change complicate wild...   \n",
       "3                    None  2018 8 million acres burned wildfire working a...   \n",
       "4                    None  2018 8 million acres burned wildfire working a...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [sad, summer, wildfire, subsequent, flash, flo...  \n",
       "1  [huge, flow, country, wildfire, doubled, emiss...  \n",
       "2  [suburban, sprawl, climate, change, complicate...  \n",
       "3  [2018, 8, million, acres, burned, wildfire, wo...  \n",
       "4  [2018, 8, million, acres, burned, wildfire, wo...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wf = df_wf.drop_duplicates(subset = 'clean_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wf = df_wf.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/df_recent.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df_wf, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(documents):\n",
    "    my_additional_stop_words = ['wildfire', 'like', 'pron']\n",
    "    stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
    "    cv_tfidf = TfidfVectorizer(analyzer = 'word',ngram_range = (2,3), min_df = 3, stop_words=stop_words, token_pattern = \"\\\\b[a-z][a-z]+\\\\b\") \n",
    "    doc_word = cv_tfidf.fit_transform(documents)\n",
    "    words = cv_tfidf.get_feature_names()\n",
    "    id2word = dict((v, k) for k, v in cv_tfidf.vocabulary_.items())\n",
    "    return doc_word, words, id2word, cv_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_lda(doc_word, n_topics):\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics,random_state=0)\n",
    "    doc_topic = lda.fit_transform(doc_word)\n",
    "    return lda, doc_topic, lda.exp_dirichlet_component_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_lsa(doc_word, no_topics):\n",
    "    '''\n",
    "    This function takes a sparse matrix map of documents to words and reduces the dimensions\n",
    "    to topics. It returns an array of documents mapped to topics by \"relatedness\". Each row in the array\n",
    "    has (no_topics) items in it.\n",
    "    --------------------\n",
    "    Inputs: sparse matrix, int\n",
    "    Outputs: model, array, list\n",
    "    '''\n",
    "    lsa = TruncatedSVD(no_topics)\n",
    "    doc_topic = lsa.fit_transform(doc_word)\n",
    "    return lsa, doc_topic, lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, words, no_top_words, topic_names=None):\n",
    "    '''\n",
    "    This function takes a dim reduction model, words, number of words to display, and topic_names \n",
    "    (default= none). It returns strings of topics.\n",
    "    ----------------\n",
    "    Input: function, list, int\n",
    "    Output: strs\n",
    "    '''\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([words[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word, words, id2word, cv_tfidf = tfidf(df_wf['clean_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda, doc_topic, variance = sk_lda(doc_word, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "artificial intelligence, california wildfires, southern california, south wales, new south wales, new south, year ago, gender reveal, company training, california million\n",
      "\n",
      "Topic  1\n",
      "california blackout, blackout crisis, california blackout crisis, blackout crisis blame, crisis blame, pg amp, los angeles, california dream, darkens california, policy totally\n",
      "\n",
      "Topic  2\n",
      "climate change, pg amp, social media, power crisis, california faces, forest service, wildfires california, sen hearing, faces fraught path, fraught path\n",
      "\n",
      "Topic  3\n",
      "kennedy peak, air quality, pg amp, gender reveals, state regulators, attendance business opportunity, rsvp ur, rsvp ur competition, opportunity rsvp ur, ur competition\n",
      "\n",
      "Topic  4\n",
      "doubled scotland, scotland emissions, doubled scotland emissions, flow country, huge flow, huge flow country, country doubled, flow country doubled, country doubled scotland, pg amp\n",
      "\n",
      "Topic  5\n",
      "home prevention, tips home, tips home prevention, help restore, acres burned, million acres, million acres burned, forests help, forests help restore, american forests\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda, words, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA works better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa, doc_topic, variance = dim_lsa(doc_word, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: ' Restoration '\n",
      "help restore, acres burned, million acres burned, american forests help, restore acres, restore acres da, acres da, working american, working american forests, acres burned working\n",
      "\n",
      "Topic: ' Scotland '\n",
      "scotland emissions, doubled scotland, doubled scotland emissions, flow country, huge flow country, huge flow, flow country doubled, country doubled, country doubled scotland, bbc news\n",
      "\n",
      "Topic: ' Home Tips '\n",
      "home prevention, tips home prevention, tips home, stay safe, warner bros, warner bros studios, bros studios, studios evacuated, studios evacuated nearby, evacuated nearby\n",
      "\n",
      "Topic: ' PG&E '\n",
      "pg amp, compensation victims, billion compensation, billion compensation victims, amp offering, pg amp offering, offering billion, amp offering billion, offering billion compensation, amp offer\n",
      "\n",
      "Topic: ' Ca Forest Policy '\n",
      "policy totally, totally backfired, policy totally backfired, california policy totally, california policy, totally backfired native, know fix, backfired native, communities know, backfired native communities\n",
      "\n",
      "Topic: ' Climate Change '\n",
      "climate change, air pollution, change increasing, climate change increasing, climate change making, change making, change makes, climate change makes, california wildfires, california problem\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, words, 10, topic_names = ['Restoration', 'Scotland', 'Home Tips',\n",
    "                                             'PG&E', 'Ca Forest Policy', 'Climate Change'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>users</th>\n",
       "      <th>followers</th>\n",
       "      <th>location</th>\n",
       "      <th>emojis</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-19 18:15:35</td>\n",
       "      <td>sad. because of this summer's wildfire and sub...</td>\n",
       "      <td>RyanBernhart_Wx</td>\n",
       "      <td>36</td>\n",
       "      <td>Maricopa, AZ</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>sad summer wildfire subsequent flash flooding ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-11-19 18:15:16</td>\n",
       "      <td>huge flow country wildfire 'doubled #scotland'...</td>\n",
       "      <td>CWL_BeGreen</td>\n",
       "      <td>1043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[(36:45) =&gt; #scotland]</td>\n",
       "      <td>huge flow country wildfire doubled emissions m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-11-19 18:14:42</td>\n",
       "      <td>suburban sprawl and climate change complicate ...</td>\n",
       "      <td>babday</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>suburban sprawl climate change complicate wild...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019-11-19 18:13:39</td>\n",
       "      <td>in 2018, over 8 million acres were burned by w...</td>\n",
       "      <td>FavaFinancialGr</td>\n",
       "      <td>188</td>\n",
       "      <td>Totowa,  New Jersey</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>million acres burned wildfire working american...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019-11-19 18:13:37</td>\n",
       "      <td>in 2018, over 8 million acres were burned by w...</td>\n",
       "      <td>ElkAgencyIns</td>\n",
       "      <td>12</td>\n",
       "      <td>Elk River, Minnesota</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>million acres burned wildfire working american...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                              tweet  \\\n",
       "0  2019-11-19 18:15:35  sad. because of this summer's wildfire and sub...   \n",
       "1  2019-11-19 18:15:16  huge flow country wildfire 'doubled #scotland'...   \n",
       "2  2019-11-19 18:14:42  suburban sprawl and climate change complicate ...   \n",
       "3  2019-11-19 18:13:39  in 2018, over 8 million acres were burned by w...   \n",
       "4  2019-11-19 18:13:37  in 2018, over 8 million acres were burned by w...   \n",
       "\n",
       "             users  followers              location emojis  \\\n",
       "0  RyanBernhart_Wx         36          Maricopa, AZ   None   \n",
       "1      CWL_BeGreen       1043                   NaN   None   \n",
       "2           babday          2                   NaN   None   \n",
       "3  FavaFinancialGr        188   Totowa,  New Jersey   None   \n",
       "4     ElkAgencyIns         12  Elk River, Minnesota   None   \n",
       "\n",
       "                 hashtags                                        clean_tweet  \n",
       "0                    None  sad summer wildfire subsequent flash flooding ...  \n",
       "1  [(36:45) => #scotland]  huge flow country wildfire doubled emissions m...  \n",
       "2                    None  suburban sprawl climate change complicate wild...  \n",
       "3                    None  million acres burned wildfire working american...  \n",
       "4                    None  million acres burned wildfire working american...  "
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying GSDMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsdmm import MovieGroupProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 7249 clusters with 15 clusters populated\n",
      "In stage 1: transferred 3534 clusters with 15 clusters populated\n",
      "In stage 2: transferred 2184 clusters with 15 clusters populated\n",
      "In stage 3: transferred 1687 clusters with 15 clusters populated\n",
      "In stage 4: transferred 1406 clusters with 15 clusters populated\n",
      "In stage 5: transferred 1290 clusters with 15 clusters populated\n",
      "In stage 6: transferred 1272 clusters with 15 clusters populated\n",
      "In stage 7: transferred 1257 clusters with 15 clusters populated\n",
      "In stage 8: transferred 1164 clusters with 15 clusters populated\n",
      "In stage 9: transferred 1144 clusters with 15 clusters populated\n",
      "In stage 10: transferred 1143 clusters with 15 clusters populated\n",
      "In stage 11: transferred 1110 clusters with 15 clusters populated\n",
      "In stage 12: transferred 1090 clusters with 15 clusters populated\n",
      "In stage 13: transferred 1099 clusters with 15 clusters populated\n",
      "In stage 14: transferred 1067 clusters with 15 clusters populated\n",
      "In stage 15: transferred 1019 clusters with 15 clusters populated\n",
      "In stage 16: transferred 1041 clusters with 15 clusters populated\n",
      "In stage 17: transferred 1017 clusters with 15 clusters populated\n",
      "In stage 18: transferred 1010 clusters with 15 clusters populated\n",
      "In stage 19: transferred 1006 clusters with 15 clusters populated\n",
      "In stage 20: transferred 1022 clusters with 15 clusters populated\n",
      "In stage 21: transferred 995 clusters with 15 clusters populated\n",
      "In stage 22: transferred 993 clusters with 15 clusters populated\n",
      "In stage 23: transferred 961 clusters with 15 clusters populated\n",
      "In stage 24: transferred 996 clusters with 15 clusters populated\n",
      "In stage 25: transferred 994 clusters with 15 clusters populated\n",
      "In stage 26: transferred 1016 clusters with 15 clusters populated\n",
      "In stage 27: transferred 971 clusters with 15 clusters populated\n",
      "In stage 28: transferred 983 clusters with 15 clusters populated\n",
      "In stage 29: transferred 1026 clusters with 15 clusters populated\n"
     ]
    }
   ],
   "source": [
    "# Train STTM model\n",
    "# Init of the Gibbs Sampling Dirichlet Mixture Model algorithm\n",
    "# K = number of potential topic (which we don't know a priori)\n",
    "# alpha = \n",
    "# beta = \n",
    "# n_iters = number of iterations to \n",
    "mgp = MovieGroupProcess(K=15, alpha=0.1, beta=0.1, n_iters=30)\n",
    "vocab = words\n",
    "n_terms = len(vocab)\n",
    "y = mgp.fit(tokens, n_terms)\n",
    "# Save model\n",
    "with open('trained_models/gsdmm_rec.pickle', 'wb') as f:\n",
    "    pickle.dump(mgp, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(num_top_words, topic_names = None):\n",
    "    cluster_lst =[mgp.choose_best_label(x) for x in tokens]\n",
    "    dict_clusters = defaultdict(list)\n",
    "    for i,v in enumerate(cluster_lst):\n",
    "        dict_clusters[str(v[0])].append(df_wf.loc[i,'clean_tweet'])\n",
    "    dict_counts = defaultdict(int)\n",
    "    ix = 0\n",
    "    for key, value in dict_clusters.items():\n",
    "        try:\n",
    "            doc_word, words, id2word, cv_tfidf = tfidf(value)\n",
    "            new = doc_word.toarray().sum(axis = 0)\n",
    "        except:\n",
    "            new = 'empty'\n",
    "        if new == 'empty':\n",
    "            print(\"\\nTopic \", key)\n",
    "        elif not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", key)\n",
    "            print(\", \".join([words[i]\n",
    "                        for i in new.argsort()[:-num_top_words - 1:-1]]))\n",
    "        else:\n",
    "            print('\\nTopic: {}'.format(topic_names[ix]))\n",
    "            print(\", \".join([words[i]\n",
    "                        for i in new.argsort()[:-num_top_words - 1:-1]]))\n",
    "        ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 852, 1375, 0, 0, 0, 0, 0, 6398, 258]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mgp.cluster_doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic : [ 635 1367  759  615 1286  645  761  858 1597  360]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [8 1 4 7 6 2 5 0 3 9]\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  4\n",
      "home prevention, tips home, tips home prevention, climate change, pg amp, long term, air quality, social media, stay safe, risk analysis, california wildfires, red cross, issues california, southern california, mitigation awards\n",
      "\n",
      "Topic  3\n",
      "doubled scotland, flow country, scotland emissions, doubled scotland emissions, huge flow, huge flow country, country doubled, flow country doubled, country doubled scotland, blackout crisis, california blackout crisis, california blackout, blackout crisis blame, crisis blame, bbc news\n",
      "\n",
      "Topic  5\n",
      "kennedy peak, pg amp, update nov, national park, studios evacuated, nov martinez, update nov martinez, entering planetary, earth entering planetary, earth entering, evacuated nearby, studios evacuated nearby, bros studios evacuated, warner bros, warner bros studios\n",
      "\n",
      "Topic  9\n",
      "acres burned, burned working, million acres burned, million acres, help restore acres, help restore, restore acres, restore acres da, forests help, burned working american, forests help restore, acres da, working american, acres burned working, american forests\n",
      "\n",
      "Topic  0\n",
      "policy totally backfired, policy totally, totally backfired, california policy, california policy totally, totally backfired native, backfired native, know fix, communities know, communities know fix, native communities know, native communities, backfired native communities, hdds event, ingested hdds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  8\n",
      "gender reveal, years ago, insight cards, social media, rick grimes, gender reveals, humor cards, climate change, integration prevention, stay safe, burn baby, christmas song, life cards, love life, pg amp\n",
      "\n",
      "Topic  1\n",
      "australia having, having cataclysmic, australia having cataclysmic, having cataclysmic season, cataclysmic season, trump regret syndrome, trump regret, regret syndrome, regret syndrome spreading, syndrome spreading, attendance business, rsvp ur competition, rsvp ur, business opportunity rsvp, business opportunity\n",
      "\n",
      "Topic  7\n",
      "pg amp, artificial intelligence, los angeles, company training, million acres, training artificial intelligence, training artificial, california million, california million acres, artificial intelligence scour, intelligence scour, year ago, acres forest, million acres forest, company training artificial\n",
      "\n",
      "Topic  2\n",
      "climate change, kamala harris, state emergency, costs climb, companies flagging risk, risk suppression, risk suppression costs, companies flagging, flagging risk suppression, suppression costs, flagging risk, suppression costs climb, los angeles, declared state, sen kamala harris\n",
      "\n",
      "Topic  6\n",
      "pg amp, compensation victims, billion compensation, billion compensation victims, amp offering, pg amp offering, offering billion, amp offering billion, threat cut, darkens california, california dream, dream residents, california dream residents, offering billion compensation, threat darkens california\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "print('*'*20)\n",
    "# Topics sorted by the number of document they are allocated to\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "print('*'*20)\n",
    "# Show the top 5 words in term frequency for each cluster \n",
    "\n",
    "top_words(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic : [ 665  943  449  812  780  614  499 1308  486  392  582  203  388  340\n",
      "  422]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [ 7  1  3  4  0  5 10  6  8  2]\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "climate change, new technology, planetary age, earth entering planetary, earth entering, entering planetary, companies flagging risk, companies flagging, flagging risk, risk suppression costs, suppression costs climb, costs climb, risk suppression, flagging risk suppression, suppression costs\n",
      "\n",
      "Topic  6\n",
      "doubled scotland, scotland emissions, doubled scotland emissions, flow country, huge flow country, huge flow, country doubled, flow country doubled, country doubled scotland, bbc news, news huge flow, news huge, bbc news huge, wine country, return california\n",
      "\n",
      "Topic  8\n",
      "pg amp, threat cut, gavin newsom, reduce risk, gov gavin newsom, gov gavin, newsom punches, gavin newsom punches, cut aid, threat cut aid, newsom punches trump, punches trump, southern california, punches trump threat, trump threat cut\n",
      "\n",
      "Topic  11\n",
      "working american forests, burned working, working american, help restore acres, help restore, forests help restore, forests help, million acres burned, restore acres, restore acres da, burned working american, million acres, acres burned, american forests help, american forests\n",
      "\n",
      "Topic  13\n",
      "blackout crisis, blackout crisis blame, crisis blame, california blackout crisis, california blackout, sto technician, totally backfired, policy totally backfired, policy totally, california policy, california policy totally, backfired native, know fix, totally backfired native, backfired native communities\n",
      "\n",
      "Topic  5\n",
      "pg amp, hazard reduction, regulators amid, regulators amid fears, deliberate blackouts probed, deliberate blackouts, state regulators, state regulators amid, pg amp deliberate, blackouts probed state, blackouts probed, probed state, probed state regulators, amp deliberate blackouts, amp deliberate\n",
      "\n",
      "Topic  4\n",
      "gender reveal, ingested hdds event, ingested hdds, hdds event, gender reveals, humans illuminated, plane crash, defensible space, planet ingested, planet ingested hdds, scenes planet, scenes planet ingested, new normal, hong kong, smokey bear\n",
      "\n",
      "Topic  7\n",
      "social media, years ago, pg amp, insight cards, humor cards, burn baby, climate change, instrumental music, little bit, inside lungs, rick grimes, help people, inside soul, spread lik, know going\n",
      "\n",
      "Topic  2\n",
      "tips home prevention, home prevention, tips home, clint eastwood, warner bros, studios evacuated, national park, clint eastwood refuses, eastwood refuses, bros studios, warner bros studios, bros studios evacuated, studios evacuated nearby, evacuated nearby, refuses leave\n",
      "\n",
      "Topic  10\n",
      "town hall, stay safe, climate change, pg amp, researchers probe, reduce risks, fort mcmurray, probe toxic, researchers probe toxic, toxic soup, probe toxic soup, wildland urban, power shutoffs, united states, rainy season\n",
      "\n",
      "Topic  3\n",
      "trump regret, regret syndrome, trump regret syndrome, regret syndrome spreading, syndrome spreading, provides insight behavior, provides insight, insight behavior, looking forward, happy birthday, dynamics provides insight, dynamics provides, fluid dynamics provides, fluid dynamics, interesting proposal released\n",
      "\n",
      "Topic  1\n",
      "climate change, south wales, new south wales, new south, climate crisis, year ago, prone areas, pg amp, california history, past years, years ago, california deadliest, high risk, fires california, northern california\n",
      "\n",
      "Topic  12\n",
      "pg amp, compensation victims, billion compensation, billion compensation victims, amp offering, pg amp offering, offering billion, amp offering billion, offering billion compensation, amp offer, pg amp offer, compensation victims bloomberg, victims bloomberg, offer billion compensation, offer billion\n",
      "\n",
      "Topic  14\n",
      "dream residents, california dream, california dream residents, darkens california dream, darkens california, threat darkens, threat darkens california, great horned, great horned owl, horned owl, los angeles, california faces, power crisis, firefighters save, faces fraught path\n",
      "\n",
      "Topic  9\n",
      "having cataclysmic, australia having, australia having cataclysmic, cataclysmic season, having cataclysmic season, artificial intelligence, kennedy peak, company training, california million, training artificial intelligence, training artificial, artificial intelligence scour, intelligence scour, million acres forest, million acres\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n",
      "/Users/brittb28/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "print('*'*20)\n",
    "# Topics sorted by the number of document they are allocated to\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "print('*'*20)\n",
    "# Show the top 5 words in term frequency for each cluster \n",
    "\n",
    "top_words(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
